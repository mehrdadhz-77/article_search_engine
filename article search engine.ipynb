{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j23Yi5X7_-J"
      },
      "source": [
        "# __Task__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh5x0Ouq8BjJ"
      },
      "source": [
        "Using the pre-processed data in this notebook, we will try to identify the article that has contributed the most to a particular field of study based on the citation relationships between the articles based on degree centrality and pagerank."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5TQ2IXYBr-V"
      },
      "source": [
        "# __Import packages__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZwNXSV1oO4z"
      },
      "source": [
        "Mounting the drive on google colab and importing necessary packages to the runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI76ppxWCST7",
        "outputId": "9ed3b211-6a3e-4d1d-cff3-6c96ca1e78d3"
      },
      "outputs": [],
      "source": [
        "# Mouting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setting the path to the directory of the data\n",
        "root_path = '<path to the root path>'\n",
        "%cd $root_path\n",
        "\n",
        "# Importing necessary packages\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbI6wdG49z8M",
        "outputId": "3913b646-7ad0-4448-b811-f07b5f2e9f2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=39558bbb94718432a4968bf126cc9011ffcbde4686c01da7d83421bbe51129dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# Installing the pyspark for the runtime\n",
        "!pip install pyspark\n",
        "\n",
        "# Importing the pyspark to the notebook\n",
        "import pyspark\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Importing the package necessary to start a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Importing a function in order to be able to collect a list\n",
        "# after groupby\n",
        "from pyspark.sql.functions import collect_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOLcU9ub99lD"
      },
      "source": [
        "# __Spark Session__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk3a3YMa95tU"
      },
      "outputs": [],
      "source": [
        "# Create a spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.executor.memory\", \"40g\") \\\n",
        "    .config(\"spark.driver.memory\", \"50g\") \\\n",
        "    .config(\"spark.executor.cores\", \"8\") \\\n",
        "    .config(\"spark.default.parallelism\", \"20\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIThuTcF-W-g"
      },
      "source": [
        "# __Data loading__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q0qIBPbDtEY"
      },
      "source": [
        "In the following sections we will load the pre-processed data regradin the articles' information and the citation relationships to our runtime from the constructed .csv files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcKNHTrDSmTz"
      },
      "source": [
        "## __Citations__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9Tx-IhMSP9u"
      },
      "source": [
        "Here we will load the citation relationship between the articles from the Citations.csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK9ctNnR-BsK",
        "outputId": "7c701d52-4056-41c4-d72e-73accf257c65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Main: long (nullable = true)\n",
            " |-- Reference: long (nullable = true)\n",
            "\n",
            "CPU times: user 79.8 ms, sys: 7.5 ms, total: 87.3 ms\n",
            "Wall time: 12.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Lazy load of the citation data\n",
        "Citations_df = spark.read.csv(f'{root_path}Filtered_citations.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Cache the data once loaded\n",
        "Citations_df.cache()\n",
        "\n",
        "# Report the structure of the dataframe\n",
        "Citations_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1byhyKaTC4B"
      },
      "source": [
        "Checking the number of the citations in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx-1ipKYINBR",
        "outputId": "805f7094-f421-4199-8030-fba9484bef1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of the edges is:  5635143\n",
            "CPU times: user 34.5 ms, sys: 3.82 ms, total: 38.4 ms\n",
            "Wall time: 5.31 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Report the number of the records in the citations dataframe\n",
        "print('The number of the edges is: ', Citations_df.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2UMISXZTM-U"
      },
      "source": [
        "__Note__: For the first time the dataframe would be cached to the memory here, as spark would store the data after explicitly loading the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeMui3lCSoRu"
      },
      "source": [
        "## __Articles__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhjfzH3Y_F_X"
      },
      "source": [
        "Reading data containing the id and the title of the articles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pir939fb-PED",
        "outputId": "91a9eaa7-62f0-406e-ad50-f76f3d4bd80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            "\n",
            "CPU times: user 5.89 ms, sys: 1.12 ms, total: 7 ms\n",
            "Wall time: 616 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Lazy load of the articles' info\n",
        "Articles_df = spark.read.option('header', 'true').csv(f'{root_path}Filtered_Articles.csv')\n",
        "\n",
        "# Cache the data once loaded\n",
        "Articles_df.cache()\n",
        "\n",
        "# Report the structure of the dataframe\n",
        "Articles_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNSwijm6Tars"
      },
      "source": [
        "Checking the number of the articles that we have their information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndGWRC70ISJ5",
        "outputId": "4b597607-75af-471a-d913-de842eec0aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of the article is:  500035\n",
            "CPU times: user 18.7 ms, sys: 71 µs, total: 18.8 ms\n",
            "Wall time: 2.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Report the number of the records in the article dataframe\n",
        "Articles_num = Articles_df.count()\n",
        "print('The number of the article is: ', Articles_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ5-7izmTfPa"
      },
      "source": [
        "Checking the number of the articles that are present in our network based on the citation relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv5qLnzQT27e",
        "outputId": "c9e66c61-21b0-419b-d605-5d8ab761d29c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of the articles based on the citation relationship:  498019\n",
            "CPU times: user 18.8 ms, sys: 3.46 ms, total: 22.2 ms\n",
            "Wall time: 2.48 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Pick the distinct article ids of the union of the main and the referenced articles\n",
        "Vertices_distinct_df = Citations_df.select('Main').union(Citations_df.select('Reference')).distinct()\n",
        "\n",
        "# Report the number of the distinct articles present in the network\n",
        "print('The number of the articles based on the citation relationship: ', Vertices_distinct_df.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNttBEIM_dxr"
      },
      "source": [
        "# __Graph Modelling__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql-q5M5-_f_W"
      },
      "source": [
        "In order to convert our data into a graph representation we would have the following structure :\n",
        "\n",
        "- __Graph type__: we have a directed graph\n",
        "\n",
        "- __Nodes__: In our graph the nodes would represent a single article\n",
        "\n",
        "- __Edges__: In our graph the edges would represent a citation relationship. Have an edge of $(\\text{Article}_1, \\text{Article}_2)$ means that $\\text{Article}_1$ has cited $\\text{Article}_2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHhuNPQnVW4x"
      },
      "source": [
        "## __Most contributed article__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXgtZS3DVa49"
      },
      "source": [
        "In this work we are trying to find the most contributed article in the domain of study. There are different algorithms that would try to find the most important node in a graph.\n",
        "\n",
        "Here we would compare the results obtained from __degree centrality__ and __pageRank__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mUDfAvB-2eN"
      },
      "source": [
        "# __Degree centrality__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdXlFfFgXD6j"
      },
      "source": [
        "In this kind of centrality measure, we would __define the importance__ of a node $v$ as the __number of the neighbors__ of node $v$.\n",
        " -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXq0kskUX42T"
      },
      "source": [
        "__Note__: as in our modelling we have a __directed graph__ we should model the importance of the nodes based on the following two metrics:\n",
        "  1. The number of the __outgoing edges__ (given citations)\n",
        "  2. The numbmer of the __incoming edges__ (received citations) - _more relevant for our work_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXD7Jx0CYfV7"
      },
      "source": [
        "## __OutDegree (#outgoing edges)__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjW8D3vzxMuh"
      },
      "source": [
        "Here we would report the articles that have given the highest number of the citations to the other articles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wTspOhvYq51",
        "outputId": "90f3ddd0-cc27-4deb-c8bf-4f4bc4041295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "|      Main|count|\n",
            "+----------+-----+\n",
            "|2895896816|  307|\n",
            "|2891004411|  291|\n",
            "|1569512051|  266|\n",
            "|2738162907|  241|\n",
            "|2506633516|  240|\n",
            "|2988916019|  228|\n",
            "|2962883549|  222|\n",
            "|2971058209|  217|\n",
            "|  47957325|  216|\n",
            "|2788653635|  214|\n",
            "|2890715498|  212|\n",
            "|2765367188|  203|\n",
            "|2609532991|  203|\n",
            "|2916332638|  200|\n",
            "|2529696250|  191|\n",
            "|2951003875|  188|\n",
            "|2600147483|  186|\n",
            "|2580175322|  178|\n",
            "|2943528199|  172|\n",
            "|2964248347|  167|\n",
            "+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "CPU times: user 15.6 ms, sys: 3.7 ms, total: 19.3 ms\n",
            "Wall time: 1.52 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing the number of times that each article has been listed in the reference list of other articles\n",
        "# and sort them based on the count in descending order\n",
        "Degree_centrality_out_df = Citations_df.groupBy(\"Main\").count().orderBy(['count'], ascending = [0])\n",
        "\n",
        "# Showing the results\n",
        "Degree_centrality_out_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IND7mYpyY5u4",
        "outputId": "9017ceb0-2007-41ca-a8ae-2e89449ac2c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Article with the highest #citations given: \n",
            "\n",
            "Article ID:  2895896816\n",
            "Article Title Deep Reinforcement Learning.\n",
            "#Citations given:  307\n",
            "\n",
            "CPU times: user 19.2 ms, sys: 2.64 ms, total: 21.8 ms\n",
            "Wall time: 1.46 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Taking the id of the article that has given the highest number of citations\n",
        "Highest_degree_centrality_out = Degree_centrality_out_df.take(1)\n",
        "\n",
        "# Fitler the information of this article from the other dataframe to get the title of the article\n",
        "Highest_degree_centrality_out_info = Articles_df.filter(Articles_df.ID == Highest_degree_centrality_out[0].Main).take(1)\n",
        "\n",
        "print('Article with the highest #citations given: \\n', )\n",
        "\n",
        "# Reporting the ID of this article\n",
        "print('Article ID: ', Highest_degree_centrality_out_info[0].ID)\n",
        "\n",
        "# Reporting the title of this article\n",
        "print('Article Title', Highest_degree_centrality_out_info[0].Title)\n",
        "\n",
        "# Reporting the number of the citations that this article has given\n",
        "print('#Citations given: ', Highest_degree_centrality_out[0]['count'], end = '\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKrHentBaH77"
      },
      "source": [
        "## __InDegree (#incoming edges)__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOndSsFcY6YT"
      },
      "source": [
        "Here we would report the articles that have the highest number of the citations received by other articles.\n",
        "\n",
        "__Note:__ this measurement would be more relavant for us as the number of the citations received by others would imply that better or more important that article is.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-pXFRSuQMqf",
        "outputId": "3c16b41e-c2d3-400c-e116-471882aa93e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "| Reference|count|\n",
            "+----------+-----+\n",
            "|2151103935|10123|\n",
            "|1686810756| 8050|\n",
            "|2153635508| 7069|\n",
            "|2194775991| 6317|\n",
            "|2064675550| 5935|\n",
            "|2618530766| 5219|\n",
            "|2949650786| 4655|\n",
            "|2133665775| 4355|\n",
            "|2117539524| 4350|\n",
            "|2102605133| 4154|\n",
            "|2099471712| 3528|\n",
            "|2168356304| 3456|\n",
            "|2095705004| 3382|\n",
            "|2162915993| 3156|\n",
            "|1903029394| 3035|\n",
            "|2031489346| 3028|\n",
            "|2163352848| 2979|\n",
            "|2100379340| 2976|\n",
            "|2129812935| 2968|\n",
            "|2250539671| 2963|\n",
            "+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "CPU times: user 13.4 ms, sys: 1.99 ms, total: 15.3 ms\n",
            "Wall time: 1.28 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing the number of times that each article has been listed in the reference list of other articles\n",
        "# and sort them based on the count in descending order\n",
        "Degree_centrality_in_df = Citations_df.groupBy(\"Reference\").count().orderBy(['count'], ascending = [0])\n",
        "\n",
        "# Showing the results\n",
        "Degree_centrality_in_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij5xzkVpRB4t"
      },
      "source": [
        "Reporting the article that has received the highest number of citations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeSTSnVeSmo6",
        "outputId": "382b5d24-4295-4e2d-c1c0-85067b8392dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Article with the highest #citations received: \n",
            "\n",
            "Article ID:  2151103935\n",
            "Article Title Distinctive Image Features from Scale-Invariant Keypoints\n",
            "#Citations received:  10123\n",
            "\n",
            "CPU times: user 15.5 ms, sys: 4.26 ms, total: 19.8 ms\n",
            "Wall time: 1.29 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Taking the id of the article that has received the highest number of citations\n",
        "Highest_degree_centrality_in = Degree_centrality_in_df.take(1)\n",
        "\n",
        "# Fitler the information of this article from the other dataframe to get the title of the article\n",
        "Highest_degree_centrality_in_info = Articles_df.filter(Articles_df.ID == Highest_degree_centrality_in[0].Reference).take(1)\n",
        "\n",
        "print('Article with the highest #citations received: \\n', )\n",
        "\n",
        "# Reporting the ID of this article\n",
        "print('Article ID: ', Highest_degree_centrality_in_info[0].ID)\n",
        "\n",
        "# Reporting the title of this article\n",
        "print('Article Title', Highest_degree_centrality_in_info[0].Title)\n",
        "\n",
        "# Reporting the number of the citations that this article has received\n",
        "print('#Citations received: ', Highest_degree_centrality_in[0]['count'], end = '\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W40Y3a84zaHK"
      },
      "source": [
        "# __PageRank__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxTZlx7fmvdE"
      },
      "source": [
        "In order to measure the importance of a node, PageRank algorithm would take into account the number of the citaitons received by an article as well as the quality of the citations received.\n",
        "\n",
        "__Note:__ we can define the __quality__ of the citation given to article $a$ as follows:\n",
        "  - If the article that cited article $a$ has __higher importance__, the quality of the citation is __higher__\n",
        "  - If the article that cited article $a$ has __lower imporatnce__, the quality of the citation is __lower__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHtZxefnzAY"
      },
      "source": [
        "#### Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA4OYIE4n9ls"
      },
      "source": [
        "PageRank algorithm follows the following steps: <br/><br/>\n",
        "  1. Give the same importance to all of the articles. Set the initial importance of the articles as:\n",
        "  \n",
        "   $$ \\text{Initial_rank} = \\frac{1}{N} $$\n",
        "\n",
        "   where $N$ is the __#articles__ in the network <br/><br/>\n",
        "\n",
        "  2. For a specific article $a$ we have to share the __old rank__ of $a$ among the articles that have been cited by $a$. So we will increase the rank of these articles by following factor:\n",
        "  \n",
        "  $$d * \\frac{\\text{old rank $a$}}{\\text{# citations by $a$}}$$\n",
        "\n",
        "  where $d \\in [0, 1]$ is the __decay factor__ which is the rate of continue reading the other articles\n",
        "\n",
        " __Note:__ if we don't have the references of article $b$ (__dangling node in graph__), we would increase the rank of all of the article by:\n",
        "\n",
        "  $$d * \\frac{\\text{old rank b}}{N}$$<br/>\n",
        "\n",
        "3. Finally we would add the following factor to the rank of all of the articles:\n",
        "\n",
        " $$\\frac{1 - d}{N}$$\n",
        "\n",
        " which is the probability of a new reader start to come into read the article.\n",
        "\n",
        "So the new rank of an article $s$ is calculated as follows:\n",
        "\n",
        "$$\\sum_{i = 0}^{k} d * \\frac{\\text{old rank }a_i}{\\text{#citations by }a_i} + \\sum_{j = 0}^{l} d * \\frac{\\text{old rank }b_j}{N} + \\frac{1-d}{N}$$\n",
        "where  $a_0, ..., a_k \\in s_{\\text{neighbors}}$ and $b_0, ..., b_l \\in \\{\\text{dangling nodes}\\}$ <br/><br/>\n",
        "\n",
        "\n",
        " __Stopping criteria:__ we should perform the steps 2, and 3 until we reach a convergence. We will reach a convergence when the rank of the articles doesn't change so much (the difference between the old an new rank is small).\n",
        "\n",
        " __Note:__ as PageRank is computationally heavy algorithm to be run, we would follow the steps for __10 iterations__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKEJ6nmJAdHQ"
      },
      "source": [
        "### PageRank implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sywkSifTagkm",
        "outputId": "3e58c19d-f3b3-4218-fcb6-ac1ddbe2679e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of the distinct nodes in the graph:  498019\n",
            "CPU times: user 1.7 s, sys: 242 ms, total: 1.94 s\n",
            "Wall time: 3min 59s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Number of the iterations\n",
        "Iterations_num = 10\n",
        "\n",
        "# Get a rdd for the edges (citation relationships) in the network\n",
        "Edges_rdd = Citations_df.rdd.map(tuple)\n",
        "\n",
        "# Get the id of the nodes (articles) in the network based on the edges in the network\n",
        "Vertices_distinct_df = Citations_df.select('Main').union(Citations_df.select('Reference')).distinct()\n",
        "\n",
        "# Count the number of the distinc vertices in the graph\n",
        "Vertices_num = Vertices_distinct_df.count()\n",
        "\n",
        "# Grouping the citations to get the number of the outgoing edges for each specific node\n",
        "Citations_grouped = Citations_df.groupBy('Main').agg(collect_list('Reference'))\n",
        "\n",
        "# For each node in the graph, put the initial rank of the node and the number of the outgoing edges\n",
        "Vertices_prob_len_rdd = Vertices_distinct_df.join(Citations_grouped, on = 'Main', how = 'outer').withColumn('rank', lit(1 / Vertices_num)).rdd.map(lambda x: (x[0], (x[2], 0 if x[1] == None else len(x[1]))))\n",
        "\n",
        "print('The number of the distinct nodes in the graph: ', Vertices_num)\n",
        "\n",
        "# Keep track of the number of the partitions in the ranking rdd\n",
        "Rank_partitions_num = Vertices_prob_len_rdd.getNumPartitions()\n",
        "\n",
        "# Probability of going to one of the references of a specific article\n",
        "damping_factor = 0.85\n",
        "\n",
        "# Simulating a markov chain process\n",
        "for i in range(Iterations_num):\n",
        "\n",
        "  # To evenly distributing the page rank of the dangling nodes\n",
        "  For_dangling_nodes = Vertices_prob_len_rdd.map(lambda x: 0 if x[1][1] != 0 else damping_factor * (x[1][0]/Vertices_num)).sum()\n",
        "\n",
        "  # The values that should be aggregated in order to compute the new page rank\n",
        "  New_page_rank_info_rdd = Edges_rdd.join(Vertices_prob_len_rdd).map(lambda x: (x[1][0], (damping_factor * (x[1][1][0] / x[1][1][1]))))\n",
        "\n",
        "  # Computing the new rank of the node that have incoming edges\n",
        "  Ranks_to_update_partial = New_page_rank_info_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "  # Rate for teleportation\n",
        "  Teleport_rate = (1 - damping_factor) / Vertices_num + For_dangling_nodes\n",
        "\n",
        "  # Updating the rank of all of the nodes in the graph\n",
        "  Vertices_prob_len_rdd = Vertices_prob_len_rdd.leftOuterJoin(Ranks_to_update_partial).map(lambda x: (x[0], (x[1][1] + Teleport_rate if x[1][1] != None else Teleport_rate, x[1][0][1] )))\n",
        "\n",
        "  # Repartitioning the RDD for the next round\n",
        "  Vertices_prob_len_rdd = Vertices_prob_len_rdd.repartition(Rank_partitions_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMbC3zzNrizz"
      },
      "source": [
        "## Highest PageRank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it4yBzHgQ7-Z",
        "outputId": "40b591f9-145b-41ec-bfef-b6f4d82fbfd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 147 ms, sys: 21.9 ms, total: 169 ms\n",
            "Wall time: 23.4 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2154952480, (0.0026920859150006884, 6))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Cache the result of the PageRank analysis\n",
        "Vertices_prob_len_rdd.cache()\n",
        "\n",
        "# Identify the article with the highest PageRank\n",
        "Highest_page_rank = Vertices_prob_len_rdd.max(key = lambda x: x[1][0])\n",
        "Highest_page_rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow2eeK1jzMpU"
      },
      "source": [
        "# __Degree centrality vs PageRank__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jAh_mNEYNSs",
        "outputId": "a77c8819-7c28-496c-df61-57f48dd8d365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paper with highest degree centrality: \n",
            "\n",
            "ID:  2151103935\n",
            "#Citations:  10123\n",
            "Rank based on PageRank algorithm:  0.0013002304003835025\n",
            "--------------------------------------------------\n",
            "Paper with the highest rank based on PageRank algorithm: \n",
            "\n",
            "ID:  2154952480\n",
            "#Citations:  296\n",
            "Rank based on the PageRank algorithm:  0.0026920859150006884\n"
          ]
        }
      ],
      "source": [
        "print('Paper with highest degree centrality: ', end = '\\n\\n')\n",
        "print('ID: ', Highest_degree_centrality_in_info[0].ID)\n",
        "print('#Citations: ', Highest_degree_centrality_in[0]['count'])\n",
        "print('Rank based on PageRank algorithm: ', Vertices_prob_len_rdd.filter(lambda x: x[0] == int(Highest_degree_centrality_in_info[0].ID)).take(1)[0][1][0])\n",
        "print('-' * 50)\n",
        "print('Paper with the highest rank based on PageRank algorithm: ', end = '\\n\\n')\n",
        "print('ID: ', Highest_page_rank[0])\n",
        "print('#Citations: ', len(Edges_rdd.filter(lambda x: x[1] == Highest_page_rank[0]).collect()))\n",
        "print('Rank based on the PageRank algorithm: ', Highest_page_rank[1][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5pJOWDBzUOA",
        "outputId": "9920c3b7-067e-46cd-f678-d70bebd06fef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The article with he highest degree centrality is:  Distinctive Image Features from Scale-Invariant Keypoints\n",
            "The article with he highest PageRank is:  Learnability and the Vapnik-Chervonenkis dimension\n"
          ]
        }
      ],
      "source": [
        "print('The article with he highest degree centrality is: ', Articles_df.filter(Articles_df.ID == 2151103935).collect()[0]['Title'])\n",
        "print('The article with he highest PageRank is: ', Articles_df.filter(Articles_df.ID == 2154952480).collect()[0]['Title'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt-WM68cr_kW"
      },
      "source": [
        "# __TF_IDF score__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aewIwn-4lE3"
      },
      "source": [
        "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It evaluates the __importance of a word__ (or term) within a document relative to a collection of documents (corpus)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE1LsEW96FWc"
      },
      "source": [
        "We can define the TF-IDF as follows:\n",
        "\n",
        "\n",
        "$$\\text{Tf_Idf (t, d, D)} = \\text{Tf (t, d) $*$ Idf (t, D)}$$\n",
        "\n",
        "Where\n",
        "\n",
        "\n",
        "$$\\text{Tf (t, d)} = \\log(1 + \\text{freq (t, d)})$$\n",
        "\n",
        "and\n",
        "\n",
        "\n",
        "$$Idf(t,D) = \\log(\\frac{N}{\\text{count} (d \\in D: t \\in d)})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9gdVLdUA17U"
      },
      "source": [
        "__Note:__ In our study, we willl consider the __Title of the articles__ as our set of documents. Then given a query, we are going to find the articles that their title matches our query better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqwiIpnZBR1o"
      },
      "source": [
        "In order to make our data to compute the TF-IDF of the terms in each we have to go through the following steps:\n",
        "\n",
        "1. __Tokenization__: we will separate the terms that exist in the title of each article\n",
        "\n",
        "2. __Removing stopword__: stopwords are the words that are really common among in the text such as 'a', 'an', 'the', ...\n",
        "\n",
        "3. __Removing punctuations__: punctuations are the symbols that exist in a text such as '.', ',' ...\n",
        "\n",
        "4. __Stemming__: stemming is the process of removing affixes (prefixes and suffixes) from words in order to obtain their root forms, also known as word stems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzm-WzvACXzL"
      },
      "source": [
        "## Importing the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTtx1sChFczw",
        "outputId": "9aeeb662-40e0-4849-fa7a-1171af2f59af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# These are the set of stopwords that we want to remove from the words\n",
        "StopWords = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sepc3SuzFtHC"
      },
      "source": [
        "Turning our dataframe to a RDD where the values are the ID of the papers and the values are the title of the papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-lcZjdjtjnb",
        "outputId": "74b576b0-5898-4d08-8280-a76f75d276ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PythonRDD[411] at RDD at PythonRDD.scala:53"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setting the ID of the papers as the keys and Titles as the values\n",
        "Article_title_rdd = Articles_df.rdd.map(lambda row: (row['ID'], row['Title']))\n",
        "\n",
        "# Cache the data once loaded\n",
        "Article_title_rdd.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4Svc-qjbcjy"
      },
      "source": [
        "A function to perform the tokenization, stopword removal, removing punctuations and stemming the word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g6IyxyZAzNA"
      },
      "outputs": [],
      "source": [
        "# tokenization, stopword removal, removing punctuations and stemming the words in the title of an article\n",
        "def Pre_processing(Title):\n",
        "  return [PorterStemmer().stem(w.lower()) for w in nltk.word_tokenize(str(Title)) if (w.lower() not in StopWords) and (w.isalpha())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiCaRX5fk4AK"
      },
      "source": [
        "## Pre processing + TF (t,d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyhLjclHbuVY"
      },
      "source": [
        "Here we are tokenize the titles as well as removing the stopwords, removing punctuations and stemming the terms in the titles.\n",
        "\n",
        "We will construct a RDD where:\n",
        "  - Keys: Article ID\n",
        "  - Value: term occurence in that article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRSUv4J5BERd",
        "outputId": "c7240195-6392-4a2b-edd7-c37a306b7e49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The terms that present in each article: \n",
            "(('2100297733', 'relight'), 1)\n",
            "(('2100297733', 'collect'), 1)\n",
            "(('2908749843', 'base'), 1)\n",
            "(('2004877333', 'decis'), 1)\n",
            "(('1501560890', 'optim'), 1)\n",
            "(('2964324415', 'quadrangul'), 1)\n",
            "(('2015363144', 'contain'), 1)\n",
            "(('2120220749', 'condit'), 1)\n",
            "(('2245001294', 'error'), 1)\n",
            "(('2902458453', 'embed'), 1)\n",
            "CPU times: user 2.11 s, sys: 949 ms, total: 3.05 s\n",
            "Wall time: 1min 10s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Performing the pre-processing: Key: Article, Value: list of words in the title\n",
        "Article_TF_rdd = Article_title_rdd.flatMapValues(Pre_processing).map(lambda x: ((x[0],x[1]), 1)).reduceByKey(lambda a, b: a + b)\n",
        "Article_TF_rdd.cache()\n",
        "\n",
        "print('The terms that present in each article: ')\n",
        "print(*Article_TF_rdd.collect()[:10], sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upWSAYDMx0_3"
      },
      "source": [
        "## Inverse Document Frequency (IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWET3lakx7hc"
      },
      "source": [
        "Here we are computing the IDF of each term among the articles' terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKegNdlell9o",
        "outputId": "fbb9a12d-c883-40d3-a5f8-a068d92a5503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The IDF of each term: \n",
            "('quadrangul', 10.031390921596127)\n",
            "('larg', 4.827883859810765)\n",
            "('cognit', 4.909322677357768)\n",
            "('energi', 4.604840263516879)\n",
            "('kernel', 5.0781279679638045)\n",
            "('beyond', 5.886094032200099)\n",
            "('concurr', 5.473693585998201)\n",
            "('polici', 5.409989540679454)\n",
            "('challeng', 4.9090516379198705)\n",
            "('process', 3.8006410776841175)\n",
            "CPU times: user 45.4 ms, sys: 11.3 ms, total: 56.7 ms\n",
            "Wall time: 2.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Calculating the IDF of the terms in the articles: Key: Term, Value: IDF value\n",
        "IDF_rdd = Article_TF_rdd.map(lambda x: (x[0][1], 1)).reduceByKey(lambda a,b: a + b).map(lambda x: (x[0], math.log(Articles_num/x[1])))\n",
        "IDF_rdd.cache()\n",
        "\n",
        "print('The IDF of each term: ')\n",
        "print(*IDF_rdd.collect()[:10], sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwaLnxObyG5M"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfOfEtf8yJWF"
      },
      "source": [
        "Here we are computing the TF-IDF of the terms in each article's title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEyzUhsdq50-",
        "outputId": "e57ae181-4732-4060-c6d4-6806a05168fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The IDF of the terms in each document: \n",
            "('2964324415', ('quadrangul', 10.031390921596127))\n",
            "('2757356930', ('quadrangul', 10.031390921596127))\n",
            "('1998130497', ('quadrangul', 10.031390921596127))\n",
            "('2076333493', ('quadrangul', 10.031390921596127))\n",
            "('1964803452', ('quadrangul', 10.031390921596127))\n",
            "('1967057670', ('quadrangul', 10.031390921596127))\n",
            "('2108857611', ('quadrangul', 10.031390921596127))\n",
            "('2810599149', ('quadrangul', 10.031390921596127))\n",
            "('2912720455', ('quadrangul', 10.031390921596127))\n",
            "('2107997516', ('quadrangul', 10.031390921596127))\n",
            "CPU times: user 1.35 s, sys: 585 ms, total: 1.94 s\n",
            "Wall time: 13.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Computing the TF_IDF for each term in each specific document\n",
        "TF_IDF_rdd = Article_TF_rdd.map(lambda x: (x[0][1], (x[0][0], x[1]))).join(IDF_rdd).map(lambda x: (x[1][0][0], (x[0], x[1][0][1] * x[1][1])))\n",
        "TF_IDF_rdd.cache()\n",
        "\n",
        "print('The IDF of the terms in each document: ')\n",
        "print(*TF_IDF_rdd.collect()[:10], sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KIit1TWCuiJ"
      },
      "source": [
        "# __Cosine similarity__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBaahBb2Cwlf"
      },
      "source": [
        "Following the computation of the TF-IDF of the terms in each document, we can use this information to retrieve the most relevant documents based on a given query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNeK8663DDwe"
      },
      "source": [
        "When we receive a query, we compute the TF-IDF of the terms in the query as well. To pre-process the query, we will follow the same steps as before (tokenization, ...)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEV3u0v7DYQS"
      },
      "source": [
        "Assuming that we have a vector of TF-IDF terms for each document (d) and query (q), we can define the cosine similarity between a query and one document as follows:\n",
        "\n",
        "$$\\text{cosine similarity (q,d)} = \\frac{\\text{dot_product}(q, d)}{||q|| * ||d||}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3LEpS4E3kF"
      },
      "source": [
        "the documents with the highest cosine similarity value will then be defined as the most relevant to our query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjl0nCdd0Rct"
      },
      "source": [
        "## Receiving the query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu-fiU40zMrV"
      },
      "source": [
        "Here we are receiving a query, follow the pre-processing to make the terms in the query applicable for our needs and compute the TF-IDF of the temrs in the given query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSel2s2OxfVh",
        "outputId": "0821bff1-5faf-4e95-b70c-f4daedf5539b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your query: Big Data Computing \n",
            "The TF_IDF of the terms in the query: \n",
            "('data', 2.8541638295856857)\n",
            "('comput', 3.495556080903046)\n",
            "('big', 5.416720551060016)\n",
            "CPU times: user 1.56 s, sys: 292 ms, total: 1.85 s\n",
            "Wall time: 1min 17s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Receiving the query\n",
        "Query = Pre_processing(input(\"Please enter your query: \"))\n",
        "\n",
        "# Computing the term frequency of the terms in the query\n",
        "Query_frequency =  spark.sparkContext.parallelize(Query).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Computing the TF_IDF of the terms in the query\n",
        "Query_TF_IDF_rdd = Query_frequency.join(IDF_rdd).map(lambda x: (x[0], x[1][0] * x[1][1]))\n",
        "\n",
        "# Storing the RDD for the future use\n",
        "Query_TF_IDF_rdd.cache()\n",
        "\n",
        "\n",
        "print('The TF_IDF of the terms in the query: ')\n",
        "print(*Query_TF_IDF_rdd.collect()[:5], sep = '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luXg0yQEzbcA"
      },
      "source": [
        "## Dot product between articles and query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSvdOUrBzjNe"
      },
      "source": [
        "Here we will compute the dot product between the articles and the query for computing the cosine similarity between the article's title and the given query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlHQiBqJa7Q5",
        "outputId": "6eb3f23a-21d3-472c-af92-0a3c62727cea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terms to article and TfIdf: \n",
            "('quadrangul', ('2964324415', 10.031390921596127))\n",
            "('quadrangul', ('2757356930', 10.031390921596127))\n",
            "('quadrangul', ('1998130497', 10.031390921596127))\n",
            "('quadrangul', ('2076333493', 10.031390921596127))\n",
            "('quadrangul', ('1964803452', 10.031390921596127))\n",
            "('quadrangul', ('1967057670', 10.031390921596127))\n",
            "('quadrangul', ('2108857611', 10.031390921596127))\n",
            "('quadrangul', ('2810599149', 10.031390921596127))\n",
            "('quadrangul', ('2912720455', 10.031390921596127))\n",
            "('quadrangul', ('2107997516', 10.031390921596127))\n",
            "\n",
            "The result of the dot product between the query and the docuemnts: \n",
            "('752888290', 8.146251166115228)\n",
            "('1546670027', 8.146251166115228)\n",
            "('2610980058', 37.48711269439115)\n",
            "('2052089314', 8.146251166115228)\n",
            "('2569038937', 8.146251166115228)\n",
            "('2980081042', 8.146251166115228)\n",
            "('2772466107', 8.146251166115228)\n",
            "('2784003762', 8.146251166115228)\n",
            "('2294319731', 8.146251166115228)\n",
            "('2912981821', 8.146251166115228)\n",
            "CPU times: user 1.53 s, sys: 605 ms, total: 2.14 s\n",
            "Wall time: 14.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Keys are the terms, values are the articles and the TfIdf of the term in that document\n",
        "terms_article_TfIdf_rdd = TF_IDF_rdd.map(lambda x: (x[1][0], (x[0], x[1][1])))\n",
        "\n",
        "print('Terms to article and TfIdf: ')\n",
        "print(*terms_article_TfIdf_rdd.collect()[:10], sep = '\\n')\n",
        "\n",
        "# Computing the dot product between the query terms and the documents\n",
        "Dot_product_query = Query_TF_IDF_rdd.join(terms_article_TfIdf_rdd).map(lambda x: (x[1][1][0], x[1][0] * x[1][1][1])).reduceByKey(lambda a,b: a + b)\n",
        "\n",
        "# Saving the RDD for the future use\n",
        "Dot_product_query.cache()\n",
        "\n",
        "print('\\nThe result of the dot product between the query and the docuemnts: ')\n",
        "print(*Dot_product_query.collect()[:10], sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7SUX6UWz3DL"
      },
      "source": [
        "## Norms of filtered articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VlsSUV_GAK6"
      },
      "source": [
        "Now we will compute the norms of the articles to compute the cosine similarity later on.\n",
        "\n",
        "__Note:__ to compute the similarity, we will filter the articles that have at least one of the terms in the given query in their title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiAj2ojR4RQu",
        "outputId": "547e4143-8ee3-4666-8a10-b429a5808f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The norms of the articles that had the terms in the query: \n",
            "('2912981821', 17.185310048167153)\n",
            "('2526069705', 18.864957245856903)\n",
            "('2489482098', 12.675352659730322)\n",
            "('2742484346', 21.675815443824522)\n",
            "('2992209313', 17.499780245109594)\n",
            "CPU times: user 4.09 s, sys: 614 ms, total: 4.7 s\n",
            "Wall time: 11min 56s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Filter the articles that have some of the terms in the query\n",
        "Filtered_articles = Dot_product_query.keys().collect()\n",
        "\n",
        "# Computing the norm of the vectors\n",
        "Articles_norm = TF_IDF_rdd.filter(lambda x: x[0] in Filtered_articles).map(lambda x: (x[0], x[1][1]**2)).reduceByKey(lambda a, b: a + b)\\\n",
        "                                  .map(lambda x: (x[0], x[1]**.5))\n",
        "\n",
        "# Storing the norms for the future use\n",
        "Articles_norm.cache()\n",
        "\n",
        "print(\"The norms of the articles that had the terms in the query: \")\n",
        "print(*Articles_norm.collect()[:5], sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqE7xdT8geRt"
      },
      "source": [
        "Now we can compute the cosine similarity between the articles and the given query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e05iGHmrhvvo",
        "outputId": "1234dd4d-6134-4657-a382-766c0e02fbfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here we have the cosine similarity between the articles' title and the given query: \n",
            "('71381689', 0.8684333216167935)\n",
            "('2519706319', 0.8684333216167935)\n",
            "('2550984048', 0.8181325769898594)\n",
            "('2406823926', 0.8181325769898594)\n",
            "('1991304154', 0.7975507516868309)\n",
            "CPU times: user 110 ms, sys: 11.4 ms, total: 121 ms\n",
            "Wall time: 6.55 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Computing the norm of the query\n",
        "Query_norm  = sum(Query_TF_IDF_rdd.map(lambda x: (x[0], x[1]**2)).values().collect())**.5\n",
        "\n",
        "# Computing the cosine similarity between the articles' title and the given query\n",
        "Cosine_similarity = Dot_product_query.join(Articles_norm).map(lambda x: (x[0], x[1][0]/(x[1][1] * Query_norm))).sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "\n",
        "print(\"Here we have the cosine similarity between the articles' title and the given query: \")\n",
        "print(*Cosine_similarity.collect()[:5], sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me-IUddGFiNt"
      },
      "source": [
        "## Top 5 articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHxSBqN2FlB3"
      },
      "source": [
        "Here we can set the top 5 articles that have the highest cosine similarity wrt to our query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzHzqW2Rixmu",
        "outputId": "5f2b1f73-ffb4-4abe-a479-05194a9c398f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 articles with the highest cosine similarity: (Article_id, (cosine_similarity, title))\n",
            "\n",
            "('71381689', (0.8684333216167935, 'Big Data – A State-of-the-Art'))\n",
            "('2519706319', (0.8684333216167935, 'Big data'))\n",
            "('2550984048', (0.8181325769898594, 'Content-Centric and Software-Defined Networking with Big Data'))\n",
            "('2406823926', (0.8181325769898594, 'Big Data over Networks'))\n",
            "('1991304154', (0.7975507516868309, 'Efficient computation of the well-founded semantics over big data'))\n",
            "CPU times: user 97.9 ms, sys: 20.6 ms, total: 118 ms\n",
            "Wall time: 6.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Sorting the results based on the cosine similarity of the article's titles and the given query\n",
        "Top_cosine_similarity = Cosine_similarity.join(Article_title_rdd).sortBy(lambda x: x[1][0], ascending=False).collect()[:5]\n",
        "\n",
        "print('Top 5 articles with the highest cosine similarity: (Article_id, (cosine_similarity, title))\\n')\n",
        "print(*Top_cosine_similarity, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW1gXYktqtGv"
      },
      "source": [
        "# __ContentLink score__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F22sQww8FyFq"
      },
      "source": [
        "We would like to have an information retrieval system that, in response to a query, not only retrieves articles that are more relevant to the query, but also considers the importance of those articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wUGlxwLGQfO"
      },
      "source": [
        "Our new metric is called the __ContentLink score__ because it combines the Cosine Similarity (a content-based score) and PageRank (a link-based score)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5qON5RWGlh4"
      },
      "source": [
        "__Note:__ before combining these two metrics, we will normalize their values to be in the range [0, 1] using Min_Max_Scaler, and then define our metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMW8gfB0HHU1"
      },
      "source": [
        "## PageRank filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu18RvUTHMHz"
      },
      "source": [
        "To obtain a fair score for the PageRank of the articles, we will filter the PageRank of the articles that contain at least one of the query terms in their title."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvbdrYj1tqHD"
      },
      "source": [
        "First we will filter the pageRank of the articles that had the terms in their title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAP7vTaJtw-X",
        "outputId": "af278ab3-8d27-467f-bcbf-f450eba7e5ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 11.8 ms, sys: 1.9 ms, total: 13.7 ms\n",
            "Wall time: 36.7 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PythonRDD[491] at RDD at PythonRDD.scala:53"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Filtering the pageRank of the articles that have at least one of the terms in their title\n",
        "Filtered_pageRank = Vertices_prob_len_rdd.filter(lambda x: str(x[0]) in Filtered_articles)\n",
        "\n",
        "#Caching the result\n",
        "Filtered_pageRank.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oczNF4hHlo-"
      },
      "source": [
        "## PageRank and Cosine similarity normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC1tm3FluDlR"
      },
      "source": [
        "Then, among the filtered articles we will find the maximum and minimum values for the pageRank and the cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDsTZOIEl218",
        "outputId": "718e2b7d-f1d2-4eef-fbc3-ee8accebf9f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 716 ms, sys: 113 ms, total: 829 ms\n",
            "Wall time: 2min 5s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Get the maximum pageRank\n",
        "Max_page_rank = Filtered_pageRank.map(lambda x: x[1][0]).reduce(max)\n",
        "\n",
        "# Get the minimum pageRank\n",
        "Min_page_rank = Filtered_pageRank.map(lambda x: x[1][0]).reduce(min)\n",
        "\n",
        "# Get the maximum cosine similarity\n",
        "Max_cosine_similarity = Cosine_similarity.map(lambda x: x[1]).reduce(max)\n",
        "\n",
        "# Get the minimum cosine similarity\n",
        "Min_cosine_similarity = Cosine_similarity.map(lambda x: x[1]).reduce(min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JQODE89tW9K"
      },
      "source": [
        "We will normalize the values using the Min_max scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95wOegQdudb-",
        "outputId": "8e1f0820-07e0-429e-b522-f9a486d996e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 42 µs, sys: 0 ns, total: 42 µs\n",
            "Wall time: 46 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Scaling the page ranks\n",
        "Scaled_pageRank = Filtered_pageRank.map(lambda x: (str(x[0]), (x[1][0] - Min_page_rank)/(Max_page_rank - Min_page_rank)))\n",
        "\n",
        "# Scaling the cosine_similarities\n",
        "Scaled_cosine_similarity = Cosine_similarity.map(lambda x: (str(x[0]), (x[1] - Min_cosine_similarity)/(Max_cosine_similarity - Min_cosine_similarity)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a2UIK0vHwak"
      },
      "source": [
        "## Defining ContentLink score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y4oSr8lwUDe"
      },
      "source": [
        "Then we can report the results based on the given query of the user with a joint metric of pageRank cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pTyGT0KwkCF"
      },
      "source": [
        "Here we will set two parameters:\n",
        "\n",
        "  - $\\alpha$: the rate of contribution of the cosine similarity\n",
        "  - $\\beta$: the rate of contribution of the pageRank\n",
        "\n",
        "We will define our __ContentLinkScore__ as:\n",
        "\n",
        " $$\\text{ContentLinkScore} = \\alpha * \\text{cosine_similarity} + \\beta * \\text{pageRank}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-QHA_yEOJta"
      },
      "source": [
        "## Different simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPLNSgcOMOp"
      },
      "source": [
        "In this section we will set the parameters of $alpha$ and $beta$ to different values and check the final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA-9aOcsOdeR"
      },
      "source": [
        "### $\\alpha = 0.5, \\beta = 0.5$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYO8h-qqOpLM"
      },
      "source": [
        "Here we are going to give the same rate of contribution to both of cosine similarity and pageRank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmDkO99eOktl",
        "outputId": "90b36201-ca72-48b5-8e0e-a2ba7754c53b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 articles with the highest ContentLinkScore: (Article_id, (ContentLinkScore, title))\n",
            "\n",
            "('2003938193', (0.6553278353064494, 'Computer Processing of Line-Drawing Images'))\n",
            "('2519706319', (0.5016216767899693, 'Big data'))\n",
            "('71381689', (0.5000859347398483, 'Big Data – A State-of-the-Art'))\n",
            "('2406823926', (0.4701387545966372, 'Big Data over Networks'))\n",
            "('2550984048', (0.47009345400788566, 'Content-Centric and Software-Defined Networking with Big Data'))\n",
            "CPU times: user 138 ms, sys: 15.7 ms, total: 154 ms\n",
            "Wall time: 9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Setting the rate of the contribution of the cosine similarity and the pageRank\n",
        "Alpha, Beta = 0.5, 0.5\n",
        "\n",
        "# Computing the ContentLinkScore\n",
        "ContentLinkScore = Scaled_pageRank.join(Scaled_cosine_similarity).map(lambda x: (x[0], x[1][0] * Beta + x[1][1] * Alpha ))\n",
        "\n",
        "# Then we will sort the articles based on their score and report the top 5\n",
        "Top_articles = ContentLinkScore.join(Article_title_rdd).sortBy(lambda x: x[1][0], ascending=False).collect()[:5]\n",
        "\n",
        "print('Top 5 articles with the highest ContentLinkScore: (Article_id, (ContentLinkScore, title))\\n')\n",
        "print(*Top_articles, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGcShkAwOT20"
      },
      "source": [
        "### $\\alpha$ = 0.4, $\\beta$ = 0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P5XHuX2yhJU"
      },
      "source": [
        "In this setting we will give more rate of contribution to the pageRank. In this case the importance of the article is more important for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06RuiWuPyaYr",
        "outputId": "f9a70225-75dc-4f9b-ddf2-3c90671e4a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 articles with the highest ContentLinkScore: (Article_id, (ContentLinkScore, title))\n",
            "\n",
            "('2003938193', (0.7242622682451595, 'Computer Processing of Line-Drawing Images'))\n",
            "('1990247061', (0.49581570180783674, 'Specification and implementation of resilient, atomic data types'))\n",
            "('2033736966', (0.4230630361722136, 'A Survey of Data Structures for Computer Graphics Systems'))\n",
            "('2041674806', (0.42179363388943775, 'Clustering categorical data: an approach based on dynamical systems'))\n",
            "('2519706319', (0.4019460121479631, 'Big data'))\n",
            "CPU times: user 125 ms, sys: 19.5 ms, total: 144 ms\n",
            "Wall time: 9.33 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Setting the rate of the contribution of the cosine similarity and the pageRank\n",
        "Alpha, Beta = 0.4, 0.6\n",
        "\n",
        "# Computing the ContentLinkScore\n",
        "ContentLinkScore = Scaled_pageRank.join(Scaled_cosine_similarity).map(lambda x: (x[0], x[1][0] * Beta + x[1][1] * Alpha ))\n",
        "\n",
        "# Then we will sort the articles based on their score and report the top 5\n",
        "Top_articles = ContentLinkScore.join(Article_title_rdd).sortBy(lambda x: x[1][0], ascending=False).collect()[:5]\n",
        "\n",
        "print('Top 5 articles with the highest ContentLinkScore: (Article_id, (ContentLinkScore, title))\\n')\n",
        "print(*Top_articles, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH0bF1ZEOci0"
      },
      "source": [
        "### $\\alpha = 0.6, \\beta = 0.4$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVVsF9UePBeX"
      },
      "source": [
        "In this setting we will give more rate of contribtuion to the cosine similarity. In this case the similarity between the document title and the query is more important for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li6NB1BPPNfp",
        "outputId": "27381f9b-b91b-4733-bd98-84e96c93dec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 articles with the highest ContentLinkScore: (Article_id, (ContentLinkScore, title))\n",
            "\n",
            "('2519706319', (0.6012973414319753, 'Big data'))\n",
            "('71381689', (0.6000687477918786, 'Big Data – A State-of-the-Art'))\n",
            "('2003938193', (0.5863934023677393, 'Computer Processing of Line-Drawing Images'))\n",
            "('2406823926', (0.5641483852804641, 'Big Data over Networks'))\n",
            "('2550984048', (0.5641121448094628, 'Content-Centric and Software-Defined Networking with Big Data'))\n",
            "CPU times: user 121 ms, sys: 19.2 ms, total: 141 ms\n",
            "Wall time: 9.15 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Setting the rate of the contribution of the cosine similarity and the pageRank\n",
        "Alpha, Beta = 0.6, 0.4\n",
        "\n",
        "# Computing the ContentLinkScore\n",
        "ContentLinkScore = Scaled_pageRank.join(Scaled_cosine_similarity).map(lambda x: (x[0], x[1][0] * Beta + x[1][1] * Alpha ))\n",
        "\n",
        "# Then we will sort the articles based on their score and report the top 5\n",
        "Top_articles = ContentLinkScore.join(Article_title_rdd).sortBy(lambda x: x[1][0], ascending=False).collect()[:5]\n",
        "\n",
        "print('Top 5 articles with the highest ContentLinkScore: (Article_id, (ContentLinkScore, title))\\n')\n",
        "print(*Top_articles, sep = '\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
